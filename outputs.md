Example link https://medium.com/the-research-nest/explained-transformers-for-everyone-af01cbe600c5

Experiment using: Simple parser with Latent Semantic Analysis (LSA)
Output:
```
'Once that’s done, you have your “AI model” to generate new responses based on everything learned from the training data. In technical terms, attention mechanisms calculate weights, determining how much focus is put on each part of the input data. This process helps the model build a deep understanding of how words are related to each other and what actually makes sense. If you want to learn more about “tokens” and “embeddings (the special numbers as I refer them),” check out my other article below. I intentionally did not touch upon the exact mathematical concepts or equations used in each layer to avoid unnecessary complications.'
```
